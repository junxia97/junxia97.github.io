!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<HTML xml:lang="en" xmlns="http://www.w3.org/1999/xhtml">

<HEAD>

<META 
content="IE=7.0000" http-equiv="X-UA-Compatible">
<TITLE>Jun Xia, Westlake University</TITLE>
<META name=keywords 
content="Jun Xia, Westlake University">
<META content=text/html;charset=utf-8 http-equiv=Content-Type>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="http://cdn.static.runoob.com/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="http://cdn.static.runoob.com/libs/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<link href="bootstrap.min.css" rel="stylesheet" media="screen" />
<link href="font-awesome-4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<LINK rel=stylesheet type=text/css  href="./jemdoc.css"><LINK rel="shortcut icon" href="rui.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
<META name=GENERATOR content="MSHTML 11.00.9600.17280"></HEAD>
<nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
            <i class="fa fa-bars"></i>
          </button>
          <div class="navbar-header my-nav">
            <a class="navbar-brand" style="margin-left: 10px" href="">Zhuosheng Zhang</a>
          </div>
        </div>
        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse navbar-right navbar-main-collapse" style="max-height: 435px; transition: max-height 0.15s ease-out;">
          <ul class="nav navbar-nav">
            <li><a href="#">Home</a></li>
            <li><a href="#Publications">Publications</a></li>
            <li><a href="#Shared">Shared Tasks</a></li>
            <li><a href="#Service">Service</a></li>
            <li><a href="#Talks">Talks</a></li>
            <li><a href="#Honors">Honors</a></li>
          </ul>
        </div>
      </div>
    </nav>

<BODY style="font-family:Raleway, Helvetica Neue, Helvetica, Arial, sans-serif;font-size:16px; padding-top: 80px">

<DIV class="container body-content">
<div class="col-6">
<P>
<!-- <script src="http://code.jquery.com/jquery-2.1.4.min.js"></script> -->
<!-- Place this tag in your head or just before your close body tag. -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script async defer src="buttons.js"></script>
<script src="jquery.scrollUp.js"></script>

<SCRIPT type=text/javascript>
  
var a = document.querySelector(".navbar-toggle");
    $(".navbar-nav li a").on("click",function () {
        a.click();
    });

// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      var x = document.getElementsByClassName("blockcontent");
      var i;
      for (i = 0; i < x.length; i++) {
          x[i].style.display = "none";
      }
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }

}

function showall(){  
    var items = document.getElementsByClassName('coauthor');
    if(document.getElementById("list_type").text == "Full list"){
        document.getElementById("list_type").text = "Selected";
        for (i = 0; i < items.length; ++i)
            {
                items[i].style.display = 'block';
            }
    }else{
        document.getElementById("list_type").text = "Full list";
        for (i = 0; i < items.length; ++i)
            {
                items[i].style.display = 'none';
            }
    }
    
}
</SCRIPT>

<script>
$(function () {
    $.scrollUp({
        animation: 'fade',
        scrollImg: {
            active: true,
            type: 'background',
            src: 'imgs/top.png'
        }
    });
});
$('#scrollUpTheme').attr('href', 'image.css?1.1');
$('.image-switch').addClass('active');

</script>

<style type="text/css">
a {
	TEXT-DECORATION: none; COLOR: #224b8d
}
H2 {
	MARGIN-BOTTOM: 0.3em;
	BORDER-BOTTOM: #aaaaaa 1px solid;
	PADDING-BOTTOM: 0.2em;
	PADDING-TOP: 0.5em;
	MARGIN-TOP: 0.7em;
	LINE-HEIGHT: 1;
}
#scrollUp {
    background-image: url("imgs/top.png");
    bottom: 20px;
    right: 20px;
    width: 38px;    /* Width of image */
    height: 38px;   /* Height of image */
}
DIV.blockcontent {
	BORDER-TOP: silver 1px solid; BORDER-RIGHT: silver 1px solid; BORDER-BOTTOM: silver 1px solid; PADDING-BOTTOM: 0.3em; PADDING-TOP: 0.3em; PADDING-LEFT: 0.5em; BORDER-LEFT: silver 1px solid; PADDING-RIGHT: 0.5em
}
pre{
border:0px;
background-color: #fff
}
@media (min-width: 1200px) {
    .container{
        width: 1000px;
    }
}

h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6 {
    font-family: inherit;
    font-weight: 360;
    line-height: 1.1;
    color: inherit;
}

</style>

</P>
<P></P>

<div class="row">
<div class="col-sm-4">
<img class="img-responsive" style="max-height:260px;" src="zzs.jpeg"/>
</div>

<div class="col-sm-8">
<div class="clearfix visible-xs-block"></div>
<h1>Zhuosheng Zhang <img style="max-height:30px;" src="images/jun_profile.png"/></h1>

<p style="font-size: 18px">Ph.D. Students<br>
School of Engineering<br>
Westlake University, Zhejiang University<br>
<b>Email</b>: <tt>&#120;&#105;&#97;&#106;&#117;&#110;&#64;&#119;&#101;&#115;&#116;&#108;&#97;&#107;&#101;&#46;&#101;&#100;&#117;&#46;&#99;&#110;</tt> <br>
<b>Advisor</b>: <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ">Stan Z. Li (IEEE Fellow)</a>    <br>

<div class="col-sm-10" style="padding-left:0px">
<div class="col-xs-2" style="padding-left:0px">
<a href="https://twitter.com/jun_westlake" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:30px;"></i></a>
</div>
<div class="col-xs-2" style="padding-left:0px">
<a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ" target="_blank" rel="noopener"><i class="ai ai-google-scholar big-icon" style=" font-size:30px;"></i></a>
</div>
<div class="col-xs-2" style="padding-left:0px">
<a href="https://github.com/junxia97" target="_blank" rel="noopener"><i class="fab fa-github big-icon" style=" font-size:30px;"></i></a>
</div>
<div class="col-xs-2" style="padding-left:0px">
<!--<a href="cv_zzs.pdf" target="_blank" rel="noopener"><i class="ai ai-cv big-icon big-icon" style=" font-size:30px;"></i></a>-->
</div>
</div>
</p>
</div>
</div>

<h2>Profile 
<!-- <a href="cv_zhangzhuosheng.pdf">[Curriculum Vitae]</a> -->
</h2>
              <p style="text-align:justify; text-justify:inter-ideograph;">Jun Xia is a Ph.D. student at Westlake University and Zhejiang University. He received his B.Eng. degree with hornurs from Central South University in 2020, advised by Chair Prof. <a style="color:#005580" href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ" target="_blank" rel="noopener">Stan Z. Li</a>. 
                His primary research interests include Graph Machine Learning, AI-driven Drug Discovery (AIDD), and Computational Biochemistry.
              
              <!-- <p style="text-align:justify; text-justify:inter-ideograph;"> His primary research interests include natural language processing, large language models, and autonomous agents. He has published over 50 papers in top-tier conferences and journals, including TPAMI, ICLR, ACL, AAAI, EMNLP, TNNLS, TASLP, and COLING. He has won 1st place in various language understanding and reasoning leaderboards, such as SQuAD2.0, MuTual, RACE, ShARC, and CMRC. He was awarded as an Academic Star at Shanghai Jiao Tong University and was selected as one of the Global Top 100 Chinese Rising Stars in Artificial Intelligence. He won the Baidu Scholarship and WAIC YunFan Award: Rising Star. -->
    
</p>

 <!--
<h2>RESEARCH INTEREST</h2>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Natural Language Understanding, Question Answering and Machine Reading Comprehension
-->
<H2>News <img src="new_gif_1.gif"></H2>
 <!--<div style="overflow-y: scroll; height:280px;">-->
 <div>
<UL>         
    <li>
        (2023.09) One paper on molecular property prediction is accepted in <strong>NeurIPS 2023</strong>.
      <li>
        (2023.09) Invited to review for ICLR 2024 and IEEE Transactions on Image Processing (TIP).
      <li>
        (2023.08) Two papers on graph clustering are accepted at <strong>ACM MM 2023</strong>.
      <li>
        (2023.06) One paper on molecular property prediction is accepted as a <strong>Spotlight Talk</strong> at ICML 2023 Computational Biology Workshop.
      <li>
        (2023.06) One paper on label-noise learning on graph data is accepted in <strong>TKDE 2023</strong>.
      <li>
        (2023.06) One paper on Chemical Pre-trained Models is accepted in <strong>ECML 2023</strong>.
     <li>
        (2023.04) One paper on  graph clustering is accepted in <strong>ICML 2023</strong>.
      <li>
        (2023.04) Our survey on Chemical Language Models has been accepted in <strong>IJCAI 2023 survey track</strong>.
     <li>
        (2023.02) Our SimGRACE (WWW 2022) paper is featured as <strong>Most Influential WWW Papers</strong> by <a href="https://www.paperdigest.org/2023/01/most-influential-www-papers-2023-01/">Paper Digest</a>. 
     <li>
        (2023.02) Two papers (One oral presentation) are accepted in <strong>CVPR 2023</strong>. 
     <li>
        (2023.02) Three papers are accepted in <strong>ICASSP 2023</strong>. 
      <li>
        (2023.01) I am a winner of Westlake Presidential Awards, the highest honor for Westlake students.
      <li>
        (2023.01) One paper on  molecular pre-trained models is accepted in <strong>ICLR 2023</strong>. 
      <li>
        (2023.01) I am invited to serve as a program committee member of ICML 2023, KDD 2023, NeurIPS 2023. 
      <li>
        (2022.10) I am awarded <strong>National Scholarship</strong>. 
      <li>
        (2022.10) One paper is accepted in <strong>Communication Biology</strong>. 
      <li>
        (2022.06) Our survey on  pre-trained molecular graph models is accepted in <strong>Ai4Science@ICML 2022</strong>. 
      <li>
        (2022.06) One paper on  graph imbalanced learning is accepted in <strong>ECML 2022</strong>. 
      <li>
        (2022.06) I am awarded ICML 2022 participation grant. 
      <li>
        (2022.05) One paper on  graph contrastive learning is accepted in <strong>ICML 2022</strong>. 
      <li>
        (2022.01) One paper on  word embeddings  is accepted in <strong>ACL 2022</strong>. 
      <li>
        (2022.01) One paper on  label noise  is accepted in <strong>ICASSP 2022</strong>.
      <li>
        (2022.01) One paper on  graph pre-training  is accepted in <strong>WWW 2022</strong>. 
             
                
</UL>
</div>


<H2>Invited Talks</H2>
<div>
<UL>     
        <li>
                2023/10: Talk "Accelerating Drug Discovery with Graph Structured Intelligence" at Fudan University. <a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/Auto-UI.pdf" target="_blank" rel="noopener">[slides]</a>
         </li>     
         <li>
                2023/05: Talk on "Chemical Pre-trained Models: Retrospect & Prospect" @ Chinese Genomics Meet-up online <a href="https://www.youtube.com/live/0kjaJ8Nv-eU?feature=share">[Vedio]</a>.
         </li>
         <li>
                2023/03: Talk on hard negative mining in GCL @ LoGs seminer (Online)<a href="https://www.bilibili.com/video/BV1T84y1v7kj/?spm_id_from=333.337.search-card.all.click">[Vedio]</a>.
         </li>
		 <li>
                2022/11: Talk on graph contrastive learning @ Aminer seminer (Online) <a href="https://www.bilibili.com/video/BV1ZT411c7Nb/?spm_id_from=333.337.search-card.all.click">[Vedio]</a>.
         </li>
         <li>
                2022/08: Talk on pre-training GNNs @ Chungbuk National University, South Korea (Online)<a href="images/review_ICML2022w.pdf">[Slides (Stay tuned)]</a>.
         </li>
         <li>
                2022/06: Talk on hard negative mining @ Prof. <a href="https://scholar.google.com/citations?user=6hA7WmUAAAAJ&hl=en">Yue Zhang</a>'s group in Westlake University<a href="images/ProGCL_ICML2022_final.pdf">.
              </li>

              <li>
                2022/05: Talk on graph contrastive learning @ Hangzhou Normal University (Online).
              </li>
              <!-- <li>
                2021/11: Talk "Machine Reading Comprehension: The Paradigm of Pre-trained Models" at <a href="https://mp.weixin.qq.com/s/j9fSGI1dLy1GXY3GMPXrkg" target="_blank" rel="noopener">MLNLP 2021</a>.
                <a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/MLNLP2021-MRC.pdf" target="_blank" rel="noopener">[slides]</a></li>
              </li>
        <li>
                2021/08: Tutorial "Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond" at IJCAI 2021, with Prof. <a style="color:#005580" href="http://bcmi.sjtu.edu.cn/~zhaohai/" target="_blank" rel="noopener">Hai Zhao</a>. <a href="http://bcmi.sjtu.edu.cn/~zhangzs/slides/IJCAI21_TT_MRC.pdf" target="_blank" rel="noopener">[slides]</a>
              </li>
        <li>
                2021/07: Talk "Machine Reading Comprehension and Dialogue Systems" at Huawei Shanghai Institute. <a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/huawei-mrc.pdf" target="_blank" rel="noopener">[slides]</a>
              </li>
        <li>
                2020/10: Talk "My Way to Reading Comprehension: Self-cognition and Persistence" at 
                <a href="http://cips-cl.org/static/CCL2020/stu-discuss.html" target="_blank" rel="noopener">CCL 2020 Student Workshop</a>.
               <a href="http://bcmi.sjtu.edu.cn/~zhangzs/slides/CCL2020_SW.pdf" target="_blank" rel="noopener">[slides]</a></li>
              <li>
                2020/05: Talk "Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond" at UofA NLP seminar on MRC.
                <a href="http://bcmi.sjtu.edu.cn/~zhangzs/slides/mrc_seminar.pdf" target="_blank" rel="noopener">[slides]</a></li>
               <LI>
               2017/10: Talk "Fine-grained Embedding for Reading Comprehension" at <a href="https://hfl-rc.github.io/cmrc2017/" target="_blank" rel="noopener">CMRC 2017 workshop in CCL 2017</a>.
                <a href="http://bcmi.sjtu.edu.cn/~zhangzs/slides/CMRC2017_SJTU-BCMI.pdf" target="_blank" rel="noopener">[slides]</a></LI>        -->
</UL>
</div>


<div id="Selected Publications" class="field" style="">
<div class="blank"></div>
<h2 id="Publications">Selected Publications</h2>
 Discover the <b><a href="pub.html" style="color:#005580">full list</a></b> | 
<a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en" target="_blank"><img src="./logos/google_scholar.png" height="20" alt="google scholar"></a> | 
<a href="https://www.semanticscholar.org/author/Jun-Xia/2069551655" target="_blank"><img src="./logos/semantic_scholar_logo.png" height="20" alt="semantic scholar"></a> | 
<a href="https://dblp.org/pid/22/3650.html" target="_blank"> <img src="./logos/dblp_logo.png" height="22" alt="dblp"></a>.<br> 
<B>[Graph Machine Learning]</B>
<UL>
    <LI>
    <B style="color: #224b8d">You Only Look at Screens: Multimodal Chain-of-Action Agents</B><BR>
      Zhuosheng Zhang, Aston Zhang.<BR>
      arXiv, 2023<BR>
      <font color="#A6192E">"Perform a task on smart phones? Train an agent using screenshots."</font> <a href="https://twitter.com/zhangzhuosheng/status/1705407434930708717" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      [<a href="https://arxiv.org/abs/2309.11436">PDF</a>]
      [<a href="javascript:toggleBibtex('autoagent_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('autoagent_bib')" target=_self>Bib</a>]
      [<a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/Auto-UI.pdf">slides</a>]
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/Auto-UI" data-icon="octicon-star" data-show-count="true" aria-label="Star Auto-UI on GitHub">Auto-UI</a></div>
        <div id=autocot_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -- leveraging a series of intermediate previous action histories and future action plans -- to help the agent decide what action to execute. We evaluate our approach on a new device-control benchmark AITW with 30K unique instructions, spanning multi-step tasks such as application operation, web searching, and web shopping. Experimental results show that Auto-UI achieves state-of-the-art performance with an action type prediction accuracy of 90% and an overall action success rate of 74%. Code is publicly available at https://github.com/cooelf/Auto-UI.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=autocot_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhang2023autoui,
  title={You Only Look at Screens: Multimodal Chain-of-Action Agents},
  author={Zhang, Zhuosheng and Zhang, Aston},
  journal={arXiv preprint arXiv:2309.11436},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Automatic Chain of Thought Prompting in Large Language Models</B><BR>
      Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola.<BR>
      ICLR, 2023<BR>
      <font color="#A6192E">"Let's think not just step by step, but also one by one."</font> <a href="https://twitter.com/astonzhangAZ/status/1579489453789581312" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      Featured in <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a> (Adopted at 400 universities from 60 countries) <br>
      [<a href="https://arxiv.org/abs/2210.03493">PDF</a>]
      [<a href="javascript:toggleBibtex('autocot_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('autocot_bib')" target=_self>Bib</a>] 
      [<a href="https://www.bilibili.com/video/BV1t8411e7Ug/?share_source=copy_web&vd_source=ddb9b075165bf2d7ecf71a83ca75744a" target=_self>bilibili</a>] 
      [<a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/CoT-zhuosheng.pdf">slides</a>]
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/amazon-research/auto-cot" data-icon="octicon-star" data-show-count="true" aria-label="Star Auto-CoT on GitHub">Auto-CoT</a></div>
        <div id=autocot_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=autocot_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{zhang2023automatic,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR 2023)},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d"> Multimodal Chain-of-Thought Reasoning in Language Models</B><BR>
      Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola.<BR>
      arXiv, 2023<BR>
      <font color="#A6192E">"Imagine learning a textbook with no figures: Multimodal-CoT surpasses humans on ScienceQA."</font> <a href="https://twitter.com/zhangzhuosheng/status/1621666014382399490" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      Featured in <a href="https://d2l.ai/" target="_blank" rel="noopener">Dive into Deep Learning</a> (Adopted at 400 universities from 60 countries) <br>
      [<a href="imgs/mm-cot-trending-social.png">Top Trending Research on paperswithcode</a>]
      [<a href="https://www.astonzhang.com/img/mm-cot-idea.png">Idea Inspiration</a>]
      [<a href="https://arxiv.org/abs/2302.00923">PDF</a>]
      [<a href="javascript:toggleBibtex('mm-cot_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('mm-cot_bib')" target=_self>Bib</a>] 
      [<a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/CoT-zhuosheng.pdf">slides</a>]
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/amazon-science/mm-cot" data-icon="octicon-star" data-show-count="true" aria-label="Star AwesomeMRC on GitHub">MM-CoT</a></div>
        <div id=mm-cot_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%->91.68%) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available at https://github.com/amazon-science/mm-cot. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=mm-cot_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhang2023multicot,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  journal={arXiv preprint arXiv:2302.00923},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">On Element-aware Automatic Summarization: Expert-writing Test Set and Chain-of-Thought Method</B><BR>
      Yiming Wang, Zhuosheng Zhang, Rui Wang.<BR>
      ACL, 2023<BR>
      <font color="#A6192E">"You really need higher-quality reference summaries to evaluate LLMs!"</font> <a href="https://twitter.com/alsaceym/status/1677903179688624128" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      [<a href="https://arxiv.org/abs/2305.13412">PDF</a>]
      [<a href="javascript:toggleBibtex('sumcot_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('sumcot_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/Alsace08/SumCoT" data-icon="octicon-star" data-show-count="true" aria-label="Star SumCoT on GitHub">SumCoT</a></div>
        <div id=sumcot_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Automatic summarization generates concise summaries that contain key ideas of source documents. As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the "Lasswell Communication Model" proposed by Lasswell (1948), allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at this https https://github.com/Alsace08/SumCoT.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=sumcot_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{wang2023element,
  title={Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method},
  author={Wang, Yiming and Zhang, Zhuosheng and Wang, Rui},
  booktitle={The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Exploring Human-Like Translation Strategy with Large Language Models</B><BR>
      Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, Xing Wang.<BR>
      arXiv, 2023<BR>
      <font color="#A6192E">Delves into LLMs' potential for mimicking human translation strategies.</font> <a href="https://twitter.com/zwhe99/status/1655939623661760517" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      [<a href="https://arxiv.org/abs/2305.04118">PDF</a>]
      [<a href="javascript:toggleBibtex('MAPS_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('MAPS_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/zwhe99/MAPS-mt" data-icon="octicon-star" data-show-count="true" aria-label="Star MAPS-mt on GitHub">MAPS-mt</a></div>
        <div id=MAPS_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Large language models (LLMs) have demonstrated impressive capabilities in general scenarios, exhibiting a level of aptitude that approaches, in some aspects even surpasses, human-level intelligence. Among their numerous skills, the translation abilities of LLMs have received considerable attention. In contrast to traditional machine translation that focuses solely on source-target mapping, LLM-based translation can potentially mimic the human translation process that takes many preparatory steps to ensure high-quality translation. This work aims to explore this possibility by proposing the MAPS framework, which stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs to first analyze the given source text and extract three aspects of translation-related knowledge: keywords, topics and relevant demonstrations to guide the translation process. To filter out the noisy and unhelpful knowledge, we employ a selection mechanism based on quality estimation. Experiments suggest that MAPS brings significant and consistent improvements over text-davinci-003 and Alpaca on eight translation directions from the latest WMT22 test sets. Our further analysis shows that the extracted knowledge is critical in resolving up to 59% of hallucination mistakes in translation. Code is available at this https https://github.com/zwhe99/MAPS-mt.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=MAPS_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{he2023exploring,
  title={Exploring Human-Like Translation Strategy with Large Language Models},
  author={He, Zhiwei and Liang, Tian and Jiao, Wenxiang and Zhang, Zhuosheng and Yang, Yujiu and Wang, Rui and Tu, Zhaopeng and Shi, Shuming and Wang, Xing},
  journal={arXiv preprint arXiv:2305.04118},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>


<UL>
    <LI>
    <B style="color: #224b8d">Self-Prompting Large Language Models for Open-Domain QA</B><BR>
      Junlong Li, Zhuosheng Zhang, Hai Zhao.<BR>
      arXiv, 2023<BR>
      <font color="#A6192E">"Free from training data and external knowledge corpus for ODQA."</font> <a href="https://twitter.com/zhangzhuosheng/status/1604720714237591553" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      [<a href="https://arxiv.org/abs/2212.08635">PDF</a>]
      [<a href="javascript:toggleBibtex('sp-cot_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('sp-cot_bib')" target=_self>Bib</a>] 
      <br>
      <!-- <div style="padding-top:5px"><a class="github-button" href="#" data-icon="octicon-star" data-show-count="true" aria-label="Star AwesomeMRC on GitHub">Self-Prompting</a></div> -->
        <div id=sp-cot_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Open-Domain Question Answering (ODQA) requires models to answer factoid questions with no context given. The common way for this task is to train models on a large-scale annotated dataset to retrieve related documents and generate answers based on these documents. In this paper, we show that the ODQA architecture can be dramatically simplified by treating Large Language Models (LLMs) as a knowledge corpus and propose a Self-Prompting framework for LLMs to perform ODQA so as to eliminate the need for training data and external knowledge corpus. Concretely, we firstly generate multiple pseudo QA pairs with background passages and one-sentence explanations for these QAs by prompting LLMs step by step and then leverage the generated QA pairs for in-context learning. Experimental results show our method surpasses previous state-of-the-art methods by +8.8 EM averagely on three widely-used ODQA datasets, and even achieves comparable performance with several retrieval-augmented fine-tuned models.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=sp-cot_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{li2022self,
  title={Self-Prompting Large Language Models for Open-Domain QA},
  author={Li, Junlong and hang, Zhuosheng and Zhao, Hai},
  journal={arXiv preprint arXiv:2212.08635},
  year={2022}
}
              </pre>
       </div>
    </LI>
</UL>


<UL>
<LI>
    <B style="color: #224b8d"> Is ChatGPT a General-Purpose Natural Language Processing Task Solver?</B><BR>
      Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, Diyi Yang<BR>
      EMNLP, 2023<BR>
      <font color="#A6192E">"Benchmarking ChatGPT on 20 popular NLP datasets covering 7 representative task categories."</font> <a href="https://twitter.com/arankomatsuzaki/status/1625312118156275712" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      [<a href="https://arxiv.org/abs/2302.06476">PDF</a>]
      [<a href="javascript:toggleBibtex('chatgpt_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('chatgpt_bib')" target=_self>Bib</a>] 
      <br>
        <div id=chatgpt_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Spurred by advancements in scale, large language models (LLMs) have demonstrated the ability to perform a variety of natural language processing (NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of ChatGPT has drawn a great deal of attention from the natural language processing (NLP) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether ChatGPT can serve as a generalist model that can perform many NLP tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of ChatGPT by evaluating it on 20 popular NLP datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of ChatGPT. We find that ChatGPT performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=chatgpt_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{qin2023chatgpt,
  title={Is ChatGPT a General-Purpose Natural Language Processing Task Solver?},
  author={Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.06476},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>





<B>[Augmented Language Models]</B>
<UL>
    <LI>
    <B style="color: #224b8d">Learning Better Masking for Better Language Model Pre-training</B><BR>
      Dongjie Yang, Zhuosheng Zhang*, Hai Zhao*.<BR>
      ACL, 2023<BR>
      <font color="#A6192E">"How will the masking ratio and masked content influence the MLM pre-training?"</font>
      [<a href="#">PDF</a>]
      [<a href="javascript:toggleBibtex('bettermask_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('bettermask_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/mutonix/better_masking" data-icon="octicon-star" data-show-count="true" aria-label="Star better_masking on GitHub">BetterMasking</a></div>
        <div id=bettermask_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=bettermask_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{yang2023learning,
  title={Learning Better Masking for Better Language Model Pre-training},
  author={Yang, Dongjie and Zhang, Zhuosheng and Zhao, Hai},
  booktitle={The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B  style="color: #224b8d"> Universal Multimodal Representation for Language Understanding </B><BR>
      Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, Hai Zhao<BR>
      TPAMI, 2023<BR>
      <font color="#A6192E">"Let's retrieve images to overcome the lack of large-scale bilingual pairs."</font> <a href="https://twitter.com/zhangzhuosheng/status/1612633454201503744" target="_blank" rel="noopener"><i class="fab fa-twitter big-icon" style=" font-size:20px;"></i></a><br> 
      [<a href="https://ieeexplore.ieee.org/document/10005816">PDF</a>]
      [<a href="javascript:toggleBibtex('uvr-pami_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('uvr-pami_bib')" target=_self>Bib</a>] 
        <div id=uvr-pami_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                               Representation learning is the foundation of natural language processing (NLP). This work presents new methods to employ visual information as assistant signals to general NLP tasks. For each sentence, we first retrieve a flexible number of images either from a light topic-image lookup table extracted over the existing sentence-image pairs or a shared cross-modal embedding space that is pre-trained on out-of-shelf text-image pairs. Then, the text and images are encoded by a Transformer encoder and convolutional neural network, respectively. The two sequences of representations are further fused by an attention layer for the interaction of the two modalities. In this study, the retrieval process is controllable and flexible. The universal visual representation overcomes the lack of large-scale bilingual sentence-image pairs. Our method can be easily applied to text-only tasks without manually annotated multimodal parallel corpora. We apply the proposed method to a wide range of natural language generation and understanding tasks, including neural machine translation, natural language inference, and semantic similarity. Experimental results show that our method is generally effective for different tasks and languages. Analysis indicates that the visual signals enrich textual representations of content words, provide fine-grained grounding information about the relationship between concepts and events, and potentially conduce to disambiguation. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=uvr-pami_bib class=blockcontent style="DISPLAY: none">
            <pre>
@ARTICLE{zhang2023universal,
  author={Zhang, Zhuosheng and Chen, Kehai and Wang, Rui and Utiyama, Masao and Sumita, Eiichiro and Li, Zuchao and Zhao, Hai},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Universal Multimodal Representation for Language Understanding}, 
  year={2023},
  volume={},
  number={},
  pages={1-18},
  doi={10.1109/TPAMI.2023.3234170}}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B  style="color: #224b8d"> SG-Net: Syntax Guided Transformer for Language Representation </B><BR>
      Zhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng Duan, Hai Zhao, Rui Wang.<BR>
      TPAMI, 2022<BR>
      [<a href="https://arxiv.org/abs/2012.13915">PDF</a>]
      [<a href="javascript:toggleBibtex('sgt_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('sgt_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/SG-Net" data-icon="octicon-star" data-show-count="true" aria-label="Star cooelf/SG-Net">SG-Net</a></div>
        <div id=sgt_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                               Understanding human language is one of the key themes of artificial intelligence. For language representation, the capacity of effectively modeling the linguistic knowledge from the detail-riddled and lengthy texts and getting rid of the noises is essential to improve its performance. Traditional attentive models attend to all words without explicit constraint, which results in inaccurate concentration on some dispensable words. In this work, we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanisms for better linguistically motivated word representations. In detail, for self-attention network (SAN) sponsored Transformer-based encoder, we introduce syntactic dependency of interest (SDOI) design into the SAN to form an SDOI-SAN with syntax-guided self-attention. Syntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation. The proposed SG-Net is applied to typical Transformer encoders. Extensive experiments on popular benchmark tasks, including machine reading comprehension, natural language inference, and neural machine translation show the effectiveness of the proposed SG-Net design. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=sgt_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhang2022sg,
  title={SG-Net: Syntax Guided Transformer for Language Representation},
  author={Zhang, Zhuosheng and Wu, Yuwei and Zhou, Junru and Duan, Sufeng and Zhao, Hai and Wang, Rui},
  journal={IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  volume={44},
  number={06},
  pages={3285--3299},
  year={2022},
  publisher={IEEE Computer Society}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B  style="color: #224b8d"> Task Compass: Scaling Multi-task Pre-training with Task Prefix </B><BR>
      Zhuosheng Zhang, Shuohang Wang, Yichong Xu, Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chenguang Zhu, Michael Zeng<BR>
      EMNLP (Findings), 2022<br>
      <font color="#A6192E">Rank 1st on the HellaSwag commonsense reasoning leaderboard & The first to achieve human parity.</font><br> 
      [<a href="https://arxiv.org/abs/2210.06277">PDF</a>]
      [<a href="javascript:toggleBibtex('emnlp_compass_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('emnlp_compass_bib')" target=_self>Bib</a>] 
      [<a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/msr-mtl.pdf" target="_blank" rel="noopener">Slides</a>]
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/CompassMTL" data-icon="octicon-star" data-show-count="true" aria-label="Star CompassMTL on GitHub">CompassMTL</a></div>
        <div id=emnlp_compass_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects across tasks. To tackle the challenge, we propose a task prefix guided multi-task pre-training framework to explore the relationships among tasks. We conduct extensive experiments on 40 datasets, which show that our model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships. The task relationships reflected by the prefixes align transfer learning performance between tasks. They also suggest directions for data augmentation with complementary tasks, which help our model achieve human-parity results on commonsense reasoning leaderboards. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id=emnlp_compass_bib class=blockcontent style="DISPLAY: none">
            <pre>
              @article{zhang2022task,
                title={Task Compass: Scaling Multi-task Pre-training with Task Prefix},
                author={Zhang, Zhuosheng and Wang, Shuohang and Xu, Yichong and Fang, Yuwei and Yu, Wenhao and Liu, Yang and Zhao, Hai and Zhu, Chenguang and Zeng, Michael},
                journal={arXiv preprint arXiv:2210.06277},
                year={2022}
              }
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B  style="color: #224b8d"> Structural Pre-training for Dialogue Comprehension </B><BR>
      Zhuosheng Zhang, Hai Zhao.<BR>
      ACL, 2021<BR>
      [<a href="https://arxiv.org/pdf/2105.10956.pdf">PDF</a>]
      [<a href="javascript:toggleBibtex('spider_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('spider_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/SPIDER" data-icon="octicon-star" data-show-count="true" aria-label="Star SPIDER on GitHub">SPIDER</a></div>
        <div id=spider_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                               Pre-trained language models (PrLMs) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training. However, even with the help of the powerful PrLMs, it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances. In this work, we present SPIDER, Structural Pre-traIned DialoguE Reader, to capture dialogue exclusive features. To simulate the dialogue-like features, we propose two training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets. Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=spider_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{zhang2021structural,
  title={Structural Pre-training for Dialogue Comprehension},
  author={Zhang, Zhuosheng and Zhao, Hai},
  booktitle={The 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021)},
  year={2021}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d"> Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese</B><BR>
      Zhuosheng Zhang, Hanqing Zhang, Keming Chen, Yuhang Guo, Jingyun Hua, Yulong Wang, Ming Zhou.<BR>
      arXiv, 2021<BR>
      [<a href="https://arxiv.org/abs/2110.06696">PDF</a>]
      [<a href="javascript:toggleBibtex('mengzi_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('mengzi_bib')" target=_self>Bib</a>] 
      [<a href="https://bcmi.sjtu.edu.cn/~zhangzs/slides/SMP-Mengzi.pdf" target="_blank" rel="noopener">Slides</a>]
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/Langboat/Mengzi" data-icon="octicon-star" data-show-count="true" aria-label="Star Mengzi on GitHub">Mengzi</a></div>
        <div id=mengzi_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Although pre-trained models (PLMs) have achieved remarkable improvements in a wide range of NLP tasks, they are expensive in terms of time and resources. This calls for the study of training more efficient models with less computation but still ensures impressive performance. Instead of pursuing a larger scale, we are committed to developing lightweight yet more powerful models trained with equal or less computation and friendly to rapid deployment. This technical report releases our pre-trained model called Mengzi, which stands for a family of discriminative, generative, domain-specific, and multimodal pre-trained model variants, capable of a wide range of language and vision tasks. Compared with public Chinese PLMs, Mengzi is simple but more powerful. Our lightweight model has achieved new state-of-the-art results on the widely-used CLUE benchmark with our optimized pre-training and fine-tuning techniques. Without modifying the model architecture, our model can be easily employed as an alternative to existing PLMs. Our sources are available at https://github.com/Langboat/Mengzi.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=mengzi_bib class=blockcontent style="DISPLAY: none">
            <pre>
@misc{zhang2021mengzi,
      title={Mengzi: Towards Lightweight yet Ingenious Pre-trained Models for Chinese}, 
      author={Zhuosheng Zhang and Hanqing Zhang and Keming Chen and Yuhang Guo and Jingyun Hua and Yulong Wang and Ming Zhou},
      year={2021},
      eprint={2110.06696},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
              </pre>
       </div>
    </LI>
</UL>


<B>[LLM Applications]</B>
<UL>
    <LI>
    <B style="color: #224b8d">Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension</B><BR>
      Xinbei Ma, Zhuosheng Zhang*, Hai Zhao*.<BR>
      TASLP, 2023<BR>
      [<a href="https://ieeexplore.ieee.org/document/10147329">PDF</a>]
      [<a href="javascript:toggleBibtex('esa_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('esa_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/xbmxb/ESA" data-icon="octicon-star" data-show-count="true" aria-label="Star ESA on GitHub">ESA</a></div>
        <div id=esa_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Multi-party multi-turn dialogue comprehension brings unprecedented challenges in handling complicated scenarios, as the co-occurrence of multiple speakers causes complexity and inconsistency. As a result of the multiple participation, the shift of speaker roles and crisscrossed discourse relations among utterances hinder reading comprehension. Motivated by this, we further integrate the enhancements of speaker-related features for dialogue comprehension performance. This work proposes a novel model with enhancement from both sides of speaker roles and speaker-aware relations. At the token level, we apply a speaker mask for attention, while at the discourse level, we utilize heterogeneous graph networks for comprehensive speaker-aware discourse clues. Experimental results show that our E nhanced S peaker- A ware method (ESA) helps achieve state-of-the-art performance on the Molweni dataset, as well as significant improvements on the FriendsQA dataset. We find that our method makes steady improvements on stronger backbones. Analysis shows that our model enhances the connections between utterances and their own speakers and captures the speaker-aware discourse relations. Discussions on data features and error cases are presented, and a visualized case is displayed. The findings reveal the importance of speaker-aware signals in dialogue comprehension.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=esa_bib class=blockcontent style="DISPLAY: none">
            <pre>
@ARTICLE{10147329,
  author={Ma, Xinbei and Zhang, Zhuosheng and Zhao, Hai},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension}, 
  year={2023},
  volume={},
  number={},
  pages={1-16},
  doi={10.1109/TASLP.2023.3284516}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d">Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification</B><BR>
      Anni Zou, Zhuosheng Zhang, Hai Zhao.<BR>
      ACL (Findings), 2023<BR>
      [<a href="https://arxiv.org/abs/2305.05921">PDF</a>]
      [<a href="javascript:toggleBibtex('decker_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('decker_bib')" target=_self>Bib</a>] 
       <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/Anni-Zou/Decker" data-icon="octicon-star" data-show-count="true" aria-label="Star Decker on GitHub">Decker</a></div>
        <div id=decker_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Commonsense fact verification, as a challenging branch of commonsense question-answering (QA), aims to verify through facts whether a given commonsense claim is correct or not. Answering commonsense questions necessitates a combination of knowledge from various levels. However, existing studies primarily rest on grasping either unstructured evidence or potential reasoning paths from structured knowledge bases, yet failing to exploit the benefits of heterogeneous knowledge simultaneously. In light of this, we propose Decker, a commonsense fact verification model that is capable of bridging heterogeneous knowledge by uncovering latent relationships between structured and unstructured knowledge. Experimental results on two commonsense fact verification benchmark datasets, CSQA2.0 and CREAK demonstrate the effectiveness of our Decker and further analysis verifies its capability to seize more precious information through reasoning.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=decker_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{zou2023decker,
  title={Decker: Double Check with Heterogeneous Knowledge for Commonsense Fact Verification},
  author={Zou, Anni and Zhang, Zhuosheng and Zhao, Hai},
  booktitle={The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023)},
  year={2023}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B  style="color: #224b8d"> Retrospective Reader for Machine Reading Comprehension </B><BR>
      Zhuosheng Zhang, Junjie Yang, Hai Zhao.<BR>
      AAAI, 2021<BR>
      <font color="#A6192E">Rank 1st on the SQuAD2.0 leaderboard.</font><BR>
      <font color="#A6192E">* Highly Influential: featured as <i>Most Influential AAAI 2021 Paper</i> (top 5) in <a href="https://www.paperdigest.org/2022/02/most-influential-aaai-papers-2022-02/" target="_blank" rel="noopener">Paper Digest</a></font><br>.
      [<a href="https://arxiv.org/abs/2001.09694">PDF</a>]
      [<a href="javascript:toggleBibtex('retro_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('retro_bib')" target=_self>Bib</a>] 
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/AwesomeMRC" data-icon="octicon-star" data-show-count="true" aria-label="Star AwesomeMRC on GitHub">Retro-Reader</a></div>
        <div id=retro_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Machine reading comprehension (MRC) is an AI challenge that requires machine to determine the correct answers to questions based on a given passage. MRC systems must not only answer question when necessary but also distinguish when no answer is available according to the given passage and then tactfully abstain from answering. When unanswerable questions are involved in the MRC task, an essential verification module called verifier is especially required in addition to the encoder, though the latest practice on MRC modeling still most benefits from adopting well pre-trained language models as the encoder block by only focusing on the "reading". This paper devotes itself to exploring better verifier design for the MRC task with unanswerable questions. Inspired by how humans solve reading comprehension questions, we proposed a retrospective reader (Retro-Reader) that integrates two stages of reading and verification strategies: 1) sketchy reading that briefly investigates the overall interactions of passage and question, and yield an initial judgment; 2) intensive reading that verifies the answer and gives the final prediction. The proposed reader is evaluated on two benchmark MRC challenge datasets SQuAD2.0 and NewsQA, achieving new state-of-the-art results. Significance tests show that our model is significantly better than the strong ELECTRA and ALBERT baselines. A series of analysis is also conducted to interpret the effectiveness of the proposed reader. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=retro_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{zhang2021retro,
  title={Retrospective Reader for Machine Reading Comprehension},
  author={Zhang, Zhuosheng and Yang, Junjie and Zhao, Hai},
  booktitle={The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI 2021)},
  year={2021}
}
              </pre>
       </div>
    </LI>
</UL>

<UL>
    <LI>
    <B style="color: #224b8d"> Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond</B><BR>
      Zhuosheng Zhang, Hai Zhao, Rui Wang.<BR>
      arXiv, 2020<BR>
      [<a href="https://arxiv.org/abs/2005.06249">PDF</a>]
      [<a href="javascript:toggleBibtex('mrcsurvey_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('mrcsurvey_bib')" target=_self>Bib</a>] 
      [<a href="http://bcmi.sjtu.edu.cn/~zhangzs/slides/mrc_seminar.pdf" target="_blank" rel="noopener">Slides</a>]
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/AwesomeMRC" data-icon="octicon-star" data-show-count="true" aria-label="Star AwesomeMRC on GitHub">AwesomeMRC</a></div>
        <div id=mrcsurvey_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                Machine reading comprehension (MRC) aims to teach machines to read and comprehend human languages, which is a long-standing goal of natural language processing (NLP). With the burst of deep neural networks and the evolution of contextualized language models (CLMs), the research of MRC has experienced two significant breakthroughs. MRC and CLM, as a phenomenon, have a great impact on the NLP community. In this survey, we provide a comprehensive and comparative review on MRC covering overall research topics about 1) the origin and development of MRC and CLM, with a particular focus on the role of CLMs; 2) the impact of MRC and CLM to the NLP community; 3) the definition, datasets, and evaluation of MRC; 4) general MRC architecture and technical methods in the view of two-stage Encoder-Decoder solving architecture from the insights of the cognitive process of humans; 5) previous highlights, emerging topics, and our empirical analysis, among which we especially focus on what works in different periods of MRC researches. We propose a full-view categorization and new taxonomies on these topics. The primary views we have arrived at are that 1) MRC boosts the progress from language processing to understanding; 2) the rapid improvement of MRC systems greatly benefits from the development of CLMs; 3) the theme of MRC is gradually moving from shallow text matching to cognitive reasoning. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=mrcsurvey_bib class=blockcontent style="DISPLAY: none">
            <pre>
@article{zhang2020mrc,
  title={Machine Reading Comprehension: The Role of Contextualized Language Models and Beyond},
  author={Zhang, Zhuosheng and Zhao, Hai and Wang, Rui},
  journal={arXiv preprint arXiv:2005.06249},
  year={2020}
}
              </pre>
       </div>
    </LI>
</UL>

<UL id="sembert">
    <LI>
    <B style="color: #224b8d"> Semantics-aware BERT for Natural Language Understanding</B><BR>
      Zhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li, Shuailiang Zhang, Xi Zhou, Xiang Zhou. <BR>
      AAAI, 2020<br>
      <font color="#A6192E">* Highly Influential: featured in the <a href="https://scholar.google.com/citations?hl=en&view_op=list_hcore&venue=PV9sQN5dnPsJ.2022&vq=eng_artificialintelligence&cstart=140" target="_blank" rel="noopener">Google Scholar Metrics 2022</a>, citations rank: 19/1591 (top 1.2%) in AAAI 2020. </font><br> 
      [<a href="https://arxiv.org/pdf/1909.02209">PDF</a>]
      [<a href="javascript:toggleBibtex('aaai2020sembert_abs')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('aaai2020sembert_bib')" target=_self>Bib</a>] 
      <!-- [<a href="https://github.com/cooelf/SemBERT">Source</a>] -->
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/SemBERT" data-icon="octicon-star" data-show-count="true" aria-label="Star cooelf/SemBERT">SemBERT</a></div>
        <div id=aaai2020sembert_abs class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                            <p style="FONT-SIZE: 16px">
                                The latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BERT precursor in a light fine-tuning way without substantial task-specific modifications. Compared with BERT, semantics-aware BERT is as simple in concept but more powerful. It obtains new state-of-the-art or substantially improves results on ten reading comprehension and language inference tasks. 
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
        <div id=aaai2020sembert_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{zhang2020semantics,
  title={Semantics-aware bert for language understanding},
  author={Zhang, Zhuosheng and Wu, Yuwei and Zhao, Hai and Li, Zuchao and Zhang, Shuailiang and Zhou, Xi and Zhou, Xiang},
  booktitle={Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI 2020)},
  volume={34},
  number={05},
  pages={9628--9635},
  year={2020}
}

              </pre>
       </div>
       
    </LI>
</UL>

<UL id="dua">
    <LI>
    <B style="color: #224b8d"> Modeling Multi-turn Conversation with Deep Utterance Aggregation</B><BR>
      Zhuosheng Zhang, Jiangtong Li, Pengfei Zhu, Hai Zhao and Gongshen Liu. <BR>
      COLING, 2018<br>
      <font color="#A6192E">* Highly Influential: featured in the <a href="https://scholar.google.com/citations?hl=en&vq=eng_computationallinguistics&view_op=list_hcore&venue=6AfzgED5a7MJ.2020" target="_blank" rel="noopener">Google Scholar 2020 h5-index list</a>, top 1.2% (4/331) in COLING 2018. </font><br> 
      [<a href="https://www.aclweb.org/anthology/C18-1317">PDF</a>]
      [<a href="javascript:toggleBibtex('dua_abstract')" target=_self>Abstract</a>]
      [<a href="javascript:toggleBibtex('dua_bib')" target=_self>Bib</a>] 
      <!-- [<a href="https://github.com/cooelf/DeepUtteranceAggregation">Source</a>] -->
      <br>
      <div style="padding-top:5px"><a class="github-button" href="https://github.com/cooelf/DeepUtteranceAggregation" data-icon="octicon-star" data-show-count="true" aria-label="Star cooelf/DeepUtteranceAggregation">DeepUtteranceAggregation</a></div>
        <div id=dua_abstract class=blockcontent style="DISPLAY: none">
            <table class=imgtable>
                <tbody>
                    <tr>
                        <td>
                                <img alt="alt text" src="pubs/dua.png"
                                     width=400>
                        </td>
                        <td>
                            <p style="FONT-SIZE: 16px">
                               Multi-turn conversation understanding is a major challenge for building intelligent dialogue systems. This work focuses on retrieval-based response matching for multi-turn conversation whose related work simply concatenates the conversation utterances, ignoring the interactions among previous utterances for context modeling. In this paper, we formulate previous utterances into context using a proposed deep utterance aggregation model to form a fine-grained context representation. In detail, a self-matching attention is first introduced to route the vital information in each utterance. Then the model matches a response with each refined utterance and the final matching score is obtained after attentive turns aggregation. Experimental results show our model outperforms the state-of-the-art methods on three multi-turn conversation benchmarks, including a newly introduced e-commerce dialogue corpus.
                            </p>
                        </td>
                    </tr>
                </tbody>
            </table>
        </div>
          
          <div id=dua_bib class=blockcontent style="DISPLAY: none">
            <pre>
@inproceedings{zhang2018dua,
                title = {Modeling Multi-turn Conversation with Deep Utterance Aggregation},
                author = {Zhang, Zhuosheng and Li, Jiangtong and Zhu, Pengfei and Zhao, Hai},
                booktitle = {Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018)},
                pages= {3740-3752},
                year = {2018},
            }
              </pre>
          </div>
    </LI>
</UL>

<H2 id="Shared">Shared Tasks</H2>
<!-- <p>(*: as the first accomplisher) </p> -->
<B style="color: #224b8d">[May 2022] HellaSwag Leaderboard on Commonsense Reasoning </B> <Br>

<UL> 
<LI>The <B>best</B> among all submissions.</LI>
[<a href="https://leaderboard.allenai.org/hellaswag/submissions/public">Leaderboard</a>]
[<a href="#">Paper</a>]
</LI>
</UL>

<B style="color: #224b8d">[January 2021] ShARC Leaderboard on Conversational Question Answering </B> <Br>

<UL> 
<LI>The <B>best</B> among all submissions.</LI>
[<a href="https://sharc-data.github.io/leaderboard.html">Leaderboard</a>]
[<a href="https://arxiv.org/abs/2012.14827">Paper</a>]
</LI>
</UL>

<B style="color: #224b8d">[September 2020] MuTual Leaderboard on Dialogue Reasoning Challenge </B> <Br>

<UL> 
<LI>The <B>best</B> among all submissions.</LI>
[<a href="https://nealcly.github.io/MuTual-leaderboard/">Leaderboard</a>]
[<a href="https://arxiv.org/pdf/2009.06504">Paper</a>]
</LI>
</UL>

<B style="color: #224b8d">[July 2019] SQuAD2.0 Leaderboard on Machine Reading Comprehension </B> <Br>

<UL> 
<LI>The <B>best</B> models for both single and ensemble settings among all submissions (2020.01). </LI>
<LI>The <B>first</B> to surpass human benchmark on both EM and F1 scores with a single model (from 2019.07-09). </LI>
<LI>The <B>first</B> time to exceed 90% F1 score with ensemble models.
<Br>
[<a href="https://rajpurkar.github.io/SQuAD-explorer/">Leaderboard</a>]
[<a href="http://arxiv.org/abs/1908.05147">Paper</a>]
[<a href="https://news.sjtu.edu.cn/zhxw/20190807/108463.html">Report</a>]
</LI>
</UL>


<B style="color: #224b8d">[March 2019] RACE Leaderboard on Machine Reading Comprehension </B> <Br>
<UL> 
<LI>The <B>best</B> among all submissions.</LI>
<LI>The <B>best</B> among all academic submissions.
<Br>
[<a href="http://www.qizhexie.com/data/RACE_leaderboard">Leaderboard</a>]
[<a href="https://arxiv.org/abs/1901.09381">Paper</a>]
[<a href="https://baijiahao.baidu.com/s?id=1627672557977953514&wfr=spider&for=pc">Report</a>]
</LI>
</UL>

<B style="color: #224b8d">[April 2019] SNLI Leaderboard on Language Inference </B>
<UL> 
<LI>The <B>best</B> among all submissions.<Br>
[<a href="https://nlp.stanford.edu/projects/snli/">Leaderboard</a>]
[<a href="https://arxiv.org/abs/1809.02794">Paper</a>]
</LI>
</LI>
</UL>

<B style="color: #224b8d">[March 2019] GLUE Leaderboard on Language Understanding </B>
<UL> 
<LI>The <B>3rd best</B> among all submissions.</LI>
<LI>The <B>best</B> among all academic submissions.
<Br>
[<a href="https://gluebenchmark.com/leaderboard">Leaderboard</a>]
[<a href="https://arxiv.org/pdf/1909.02209.pdf">Paper</a>]
</LI>
</UL>

<B style="color: #224b8d">[August 2017] Chinese Machine Reading Comprehension (CCL-CMRC 2017) </B>
<UL> 
<LI>The <B>best</B> single system and the <b>second </b> ensemble system (Silver Medal).
<Br>
[<a href="https://hfl-rc.com/cmrc2017/leaderboard/">Leaderboard</a>]
[<a href="https://www.aclweb.org/anthology/C18-1153">Paper</a>]
[<a href=" https://github.com/cooelf/subMrc">Source</a>]
</LI>
</UL>

<H2 id="Honors">Awards & Honors</H2>
<UL>
<li> <p> 2023: WAIC YunFan Award, Rising Star, World Artificial Intelligence Conference.</P>
<li> <p> 2023: Shanghai Outstanding Doctoral Graduate. <P>
<li> <p> 2022: Academic Stars of Graduate Students (10 recipients), Shanghai Jiao Tong University.</P>
<li> <p> 2021: Global Top 100 Chinese Rising Stars in Artificial Intelligence (Top 10 recommended), Baidu Research.</P>
<li> <p> 2021: Baidu Scholarship (Top-10, worldwide), Baidu.</P>
<li> <p> 2020: National Scholarship of China, Ministry of Education of the P.R. China.</P>
<li> <p> 2019: Yang Yuanqing Education Fund, The foundation of Class 1988 in CS @ Shanghai Jiao Tong University.</P>
<li> <p> 2018: Academic Stars of Graduate Students (The only master student awardee), Shanghai Jiao Tong University.</P>
<li> <p> 2016: National Figures Nomination of College Students (20 total recipients), Ministry of Education of the P.R. China.</P>
<li> <p> 2015: CCF Elite Collegiate Award, China Computer Federation.</P>


<!-- <LI>  <P><i>National Annual Figures Nomination of College Students</i>, Ministry of Education of P.R.China </P>
  <LI>  <P><i>The CCF Elite Collegiate Award</i>, China Computer Federation </P> -->
<!-- <li>  <p><i>Outstanding Bachelor Thesis Award</i>, Hubei Provincial Department of Education</p>
<LI>  <P><i>First Prize in the 2014&2015 TI Cup National Internet of Things Competition</i>, CS Committee in Ministry of Education of P.R.China</P>-->
<!--   <LI>  <P>Second Prize in the 2015 National Information Security Competition</P> -->
<!--     <LI>  <P>Excellent Award in IBM Innovation Mobile Application Challenge</P>
    <li>  <p>Third Prize of Mathematical Modeling Competition in Central China Region</p> -->
</UL>

<h2 id="Service">Teaching</h2>
<ul style="padding-left: 40px;">
	<li><b>Guest Lecturer</b>, <i>NIS8021: Frontier Technology in Natural Language Processing</i><br>
		Graduate, Shanghai Jiao Tong University, Winter 2022.
	<li><b>Teaching Assistant</b>, <i>F03356: Natural Language Understanding</i><br>
		Graduate, Shanghai Jiao Tong University, Spring 2021.
	<li><b>Teaching Assistant</b>, <i>F03356: Natural Language Understanding</i><br>
		Graduate, Shanghai Jiao Tong University, Spring 2019.
    <li><b>Teaching Assistant</b>, <i>F03356: Natural Language Understanding</i><br>
    	Graduate, Shanghai Jiao Tong University, Spring 2018.
</ul>

    
<h2 id="Service">Academic Service</h2>
<ul>
  <li> Organization: 
<ul style="padding-left: 40px;">
  	<li>Co-chair of <a href="http://cips-cl.org/static/CCL2022/en/cclCommittee/comChairs/index.html">CCL Student Seminar</a>, 2022
    <li>President of IBM Tech Club at Wuhan University, 2014-2015.
	</ul>
<li>  Area Chair: ICLR 2023 TinyPapers
    <li>  Program Committee Member:
  <ul style="padding-left: 40px;">
  	<li>ML/AI conferences: ICLR, ICML, NeurIPS, AAAI, IJCAI, etc.
    <li>CL/NLP conferences: ARR, ACL, EMNLP, COLING, NAACL, AACL, NLPCC, CCL, etc.
	<ul style="padding-left: 40px;">
	</ul>
    <p>
  </ul>
    <li> Journal Reviewer: Artificial Intelligence, IEEE/ACM TASLP, IEEE TNNLS, IEEE TETCI, IEEE Communications Magazine, ACM TALLIP, ACM TOIS, TMLR, Neurocomputing, Multimedia Systems, Neural Computing and Applications, Expert Systems With Applications.
</ul>

<h2 id="Service">Student Advising</h2>
(<i>* I am co-advising undergraduate students at SJTU with Prof. Hai Zhao.</i>)
<ul>
    <li> <a href="https://ozyyshr.github.io/">Siru Ouyang</a> (Undergraduate Student at SJTU &rarr; PhD Student at UIUC)
    <li> <a href="https://cather-chen.github.io/">Jialin Chen</a> (Undergraduate Student at SJTU &rarr; PhD Student at Yale University)
    <li> <a href="https://scholar.google.com/citations?user=JcBQtgYAAAAJ">Yuchen He</a> (Undergraduate Student at SJTU &rarr; MS Student at SJTU)
    <li> <a href="https://scholar.google.com/citations?user=UX7TpSYAAAAJ">Junlong Li</a> (Undergraduate Student at SJTU &rarr; MS Student at SJTU)
    <li> <a href="https://scholar.google.com/citations?user=tK2YD60AAAAJ">Longxiang Liu</a> (Undergraduate Student at SJTU &rarr; MS Student at ICT/CAS)
    <li> <a href="https://willyoung2017.github.io/">Yuwei Wu</a> (Undergraduate Student at SJTU &rarr; MS Student at CMU)
</ul>
<br>
<br>
<!-- 
<H2 id="ETC">Hobbies</H2>

s
<UL>
<li>  Traveling</li>
<li>  Playing video games</li>
<li>  Listening to music, playing instruments (esp. ErHu <img style="max-height:18px;" src="imgs/erhu.png"/>)</li>
</UL>
<img class="img-responsive" style="max-height:200px;"src="imgs/travel.png"/>
-->


</div>
 </DIV></BODY>


</HTML>
