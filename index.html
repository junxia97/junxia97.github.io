<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content=‚Äúwidth=800‚Äù>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
 
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14px;
    }

    strongsmall {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    smalll {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
    }

    stronghuge {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }

    huge {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 30px;
    }

    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="sjtu_icon.png">
  <title>
  </title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link rel="shortcut icon" href="image/favicon.ico" />
  <link rel="bookmark" href="image/favicon.ico" />
  <meta name="google-site-verification" content="3Pi5gRNVZ_uFXQ1gBBx91DHgGFC32ASIPVvSeEiTqz8" />
</head>

<body>
  <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Jun Xia&nbsp;&nbsp;<font face="KaiTi" size="6">Â§è‰øä</font>
                </name>
              </p>
              <p>I‚Äôm a Ph.D. student at School of Engineering, <a href="https://en.westlake.edu.cn/">Westlake University & <a href="https://www.zju.edu.cn/english/">Zhejiang University (ZJU) </a>, supervised by Chair Prof. <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=en&oi=ao">Stan Z.Li (IEEE Fellow)</a>.
                I received the B. Eng honour degree from <a href="https://en.csu.edu.cn/">Central South University (CSU)</a> in 2020.
                My current research interests include but are not limited to Graph Neural Networks, pre-training techniques and AI-asisted drug discovery. Feel free to drop me an email if you want to discuss with me.
              </p>
              <p> <strong><font color="#DA1212">üåü Hiring:</font></strong> We are always looking for research assistants, visiting students and self-motivated PhD students. All of these positions are funded. Potential candidates can contact me by email.
              </p>
                <p align=center>
                  <a href="mailto:xiajun@westlake.edu.cn">Email: xiajun [at] westlake.edu.cn</a> &nbsp/&nbsp
                  <a href="https://scholar.google.com/citations?user=aPKKpSYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                  <a href="https://github.com/junxia97">Github</a>&nbsp/&nbsp
                  <a href="https://twitter.com/JunXia_Westlake">Twitter</a>&nbsp/&nbsp
                  <a href="https://www.zhihu.com/people/xia-jun-70-77">Zhihu</a>
                </p>

            </td>
            <td width="16%">
              <img src="images/junxia.jpg" width="130">
            </td>
          </tr>
        </table>
        
     
        <p></p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>News</heading>
              <div style="line-height:25px">
                <p>
                 <li>
                    <em>(2023.02) Our SimGRACE (WWW 2022) paper is featured as <strong>Most Influential WWW Papers</strong> by <a href="https://www.paperdigest.org/2023/01/most-influential-www-papers-2023-01/">Paper Digest</a>. üòÑ
                 <li>
                    <em>(2023.02) Two papers (One oral presentation) is accepted in <strong>CVPR 2023</strong>. üòÑ
                 <li>
                    <em>(2023.02) Three papers is accepted in <strong>ICASSP 2023</strong>. üòÑ
                  <li>
                    <em>(2023.01) I am a winner of Westlake Presidential Awards, the highest honor for Westlake students. üèÜ
                  <li>
                    <em>(2023.01) One paper on <i> molecular pre-trained models<i> is accepted in <strong>ICLR 2023</strong>. üòÑ
                  <li>
                    <em>(2023.01) I am invited to serve as a program committee member of ICML 2023, KDD 2023. üòÑ
                  <li>
                    <em>(2022.10) I am awarded <strong>National Scholarship</strong>. üèÜ
                  <li>
                    <em>(2022.10) <strong>One</strong> paper is accepted in <strong>Communication Biology</strong>. üòÑ
                  <li>
                    <em>(2022.06) </em>Our survey on <i> pre-trained molecular graph models<i> is accepted in <strong>Ai4Science@ICML 2022</strong>. üòÑ
                  <li>
                    <em>(2022.06) </em><strong>One</strong> paper on <i> graph imbalanced learning<i> is accepted in <strong>ECML 2022</strong>. üòÑ
                  <li>
                    <em>(2022.06) </em> I am awarded ICML 2022 participation grant. ‚úåÔ∏è
                  <li>
                    <em>(2022.05) </em> <strong>One</strong> paper on <i> graph contrastive learning<i> is accepted in <strong>ICML 2022</strong>. üòÑ
                  <li>
                    <em>(2022.01)</em> <strong>One</strong> paper on <i> word embeddings <i> is accepted in <strong>ACL 2022</strong>. üòÑ
                  <li>
                    <em>(2022.01)</em> <strong>One</strong> paper on <i> label noise <i> is accepted in <strong>ICASSP 2022</strong>. üòÑ
                  <li>
                    <em>(2022.01)</em> <strong>One</strong> paper on <i> graph pre-training <i> is accepted in <strong>WWW 2022</strong>. üòÑ
                </p>
              </div>
            </td>
          </tr>
        </table>
                    
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Invited Talks</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    <em>(2022.11) </em>Talk on hard negative mining in GCL @ LoGs seminer (Online)<a href="https://www.bilibili.com/video/BV1T84y1v7kj/?spm_id_from=333.337.search-card.all.click">[Vedio]</a>.
                  <li>
                    <em>(2022.08) </em>Talk on graph contrastive learning @ Aminer seminer (Online) <a href="https://www.bilibili.com/video/BV1ZT411c7Nb/?spm_id_from=333.337.search-card.all.click">[Vedio]</a>.
                  <li>
                    <em>(2022.08) </em>Talk on pre-training GNNs @ Chungbuk National University, South Korea (Online)<a href="images/review_ICML2022w.pdf">[Slides (Stay tuned)]</a>.
                  <li>
                    <em>(2022.06) </em>Talk on hard negative mining @ Prof. <a href="https://scholar.google.com/citations?user=6hA7WmUAAAAJ&hl=en">Yue Zhang</a>'s group in Westlake University<a href="images/ProGCL_ICML2022_final.pdf">[Slides]</a>.
                  <li>
                  <em>(2022.05) </em>Talk on graph contrastive learning @ Hangzhou Normal University (Online).
                </p>
              </div>
            </td>
          </tr>
        </table>

        <p></p>
        <p></p>
        <p></p>
        <p></p>
        <p></p>
                    
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Selected Preprints</heading>
            </td>
          </tr>  
        </table>
         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>A Systematic Survey of Molecular Pre-trained Models</strong>
              <br><br>
              <strong><u>Jun Xia</u></strong>,
              <a>Yanqiao Zhu</a>,
              <a>Yuanqi Du</a>,
              <a>Yue Liu</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                Preprint</strong>
                <br>
              </em>
               <a href="https://arxiv.org/abs/2210.16484"><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/awesome-pretrain-on-molecules"><strong>[code]</strong></a>
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:YfcjskXf8e0J:scholar.google.com/&output=citation&scisdr=CgWsMQ24EO2_8mLFjvo:AAGBfm0AAAAAY_3DlvpH53P8F4J4XtljDj_WspezvX9f&scisig=AAGBfm0AAAAAY_3DlpQrhXP5P_jCzWDVJOsj1o_KED1X&scisf=4&ct=citation&cd=-1&hl=en"><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>                    
                    
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>  
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules</strong>
              <br><br>
              <strong><u>Jun Xia</u></strong>,
              <a>Chengshuai Zhao</a>,
              <a>Bozhen Hu</a>,
              <a>Zhangyang Gao</a>,
             <a>Cheng Tan</a>,
             <a>Yue Liu</a>,
              <a>Siyuan Li</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                ICLR 2023
                <br>
              </em>
               <a href=""><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/ProGCL"><strong>[code]</strong></a>
              <a href=""><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>
                    
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language Recognition with Variational Alignment</strong>
              <br><br>
              <u>Jiangbin Zheng</u>,
              <a>Yile Wang</a>,
              <a>Cheng Tan</a>,
              <a>Siyuan Li</a>,
             <a>Ge Wang</a>,
             <strong><a>Jun Xia</a></strong>,
              <a>Siyuan Li</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                CVPR 2023, <font color="#DA1212">Highlight (Oral) Presentation</font>
                <br>
              </em>
               <a href=""><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/ProGCL"><strong>[code]</strong></a>
              <a href=""><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>  
               
           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning</strong>
              <br><br>
              <u>Cheng Tan</u>,
              <a>Zhangyang Gao</a>,
              <a>Lirong Wu</a>,
              <a>Yongjie Xu</a>,
            <strong><a>Jun Xia</a></strong>,
              <a>Siyuan Li</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                CVPR 2023
                <br>
              </em>
               <a href=""><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/ProGCL"><strong>[code]</strong></a>
              <a href=""><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>
                   
                    
         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>ProGCL: Rethinking Hard Negative Mining in Graph Contrastive Learning</strong>
              <br><br>
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN">Lirong Wu</a>,
              <a>Ge Wang</a>,
              <a>Jintao Chen</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                ICML 2022, <font color="#DA1212">Spotlight</font>
                <br>
              </em>
               <a href="https://arxiv.org/abs/2110.02027"><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/ProGCL"><strong>[code]</strong></a>
              <a href=""><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>
        
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="100%" valign="middle">
              <strong>Pre-training Graph Neural Networks for Molecular Representations: Retrospect and Prospect</strong>

              <br><br>
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=NBbJT3AAAAAJ&hl=en">Yanqiao Zhu</a>,
              <a href="https://scholar.google.com/citations?user=fAc_zZMAAAAJ&hl=en">Yuanqi Du</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>

              <em>
                ICML 2022, AI4Science
                <br>
              </em>
               <a href="https://openreview.net/forum?id=dhXLkrY2Nj3&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3DICML.cc%2F2022%2FWorkshop%2FAI4Science%2FAuthors%23your-submissions)"><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/awesome-pretrain-on-graphs"><strong>[code]</strong></a>
              <a href=""><strong>[BibTex]</strong></a>
              <p></p>
            </td>
<!--           </tr> -->
        </table>
       
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <td width="100%" valign="middle">
              <strong>SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</strong>

              <br><br>
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN">Lirong Wu</a>,
              <a>Jintao Chen</a>,
              <a href="https://www.researchgate.net/profile/Hu-Bozhen">Bozhen Hu</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                WWW 2022, <font color="#DA1212">Most Influential WWW Papers</font> according to <a href="https://www.paperdigest.org/2023/01/most-influential-www-papers-2023-01/">Paper Digest</a>
                <br>
              </em>
               <a href="https://arxiv.org/pdf/2202.03104.pdf"><strong>[Paper]</strong></a>
               <a href="https://github.com/junxia97/SimGRACE"><strong>[code]</strong></a>
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:9zH3atwYuhcJ:scholar.google.com/&output=citation&scisdr=CgUSyJaOEKWStmTRICA:AAGBfm0AAAAAYpnUOCBg_6kn2uCTC3rL5vuvPsUVUnjD&scisig=AAGBfm0AAAAAYpnUOKcRkWc1NSD3L7FrRRx7zn8-UGmZ&scisf=4&ct=citation&cd=-1&hl=en"><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>
                    
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong> GraphMixup: Improving Class-Imbalanced Node Classification on Graphs by Self-supervised Context Prediction</strong>
              <br><br>
             <a>Lirong Wu</a>,
             <strong><u>Jun Xia</u></strong>,
             <a>Haitao Lin</a>,
              <a>Zhangyang Gao</a>,
             <a>Cheng Tan</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                 ECML 2022
                <br>
              </em>
               <a href="https://2022.ecmlpkdd.org/wp-content/uploads/2022/09/sub_375.pdf"><strong>[Paper]</strong></a>
               <a href="https://github.com/LirongWu/GraphMixup"><strong>[code]</strong></a>
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:uq5UEYCR4OIJ:scholar.google.com/&output=citation&scisdr=CgXmD_E7EPXO2d-EihI:AAGBfm0AAAAAY0aCkhIvKDIJe5af8WQ8GSaJtFMAe5XI&scisig=AAGBfm0AAAAAY0aCkkt-nZfNDnbymkyPPCwciY37FPRe&scisf=4&ct=citation&cd=-1&hl=zh-CN"><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table>
                    
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="100%" valign="middle">
              <strong>Co-learning: Learning from Noisy Labels with Self-supervision</strong>
              
              <br><br>
              <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN">Cheng Tan</a>,
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN">Lirong Wu</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                ACM MM 2021, <font color="#DA1212">Oral Presentation</font>.
                <br>
              </em>
<!--                 <strong>
                  <font color="#DA1212">(Oral Presentation)</font>
                </strong></em>  -->
                <a href="https://arxiv.org/abs/2108.04063"><strong>[Paper]</strong></a>
                <a href="https://github.com/junxia97/Co-training-based_noisy-label-learning"><strong>[Code]</strong></a>
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:3WFH3d1b2lAJ:scholar.google.com/&output=citation&scisdr=CgUSyJaOEKWStmTQUVo:AAGBfm0AAAAAYpnVSVrhlTSVpsb18_i00uA2Z1am9ScI&scisig=AAGBfm0AAAAAYpnVSTU1WLE2kGXlM9ifNYL7GvyORT3c&scisf=4&ct=citation&cd=-1&hl=zh-CN"><strong>[BibTex]</strong></a>

              <p></p>
            </td>
        </table>
                    
<!--           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>Structure-Preserving and Batch-Correcting Visualization Using Deep Manifold Transformation for Single-cell RNA-Seq Profiles</strong>
              <br><br>
             <a>Yongjie Xu</a>,
             <a>Zelin Zang</a>,
             <strong><u>Jun Xia</u></strong>,
             <a>Cheng Tan</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                 Communication Biology
                <br>
              </em>
               <a href="https://www.biorxiv.org/content/10.1101/2022.07.09.499435v1"><strong>[Paper]</strong></a>
               <a href=""><strong>[code]</strong></a>
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:6mi9Ic2qrlwJ:scholar.google.com/&output=citation&scisdr=CgXmD_E7EPXO2d-FpOU:AAGBfm0AAAAAY0aDvOXXurnuBwsQArbnBZ4imstZ35G7&scisig=AAGBfm0AAAAAY0aDvMKJasZdVM5v-yTeJZtpIa9d6_sF&scisf=4&ct=citation&cd=-1&hl=zh-CN"><strong>[BibTex]</strong></a>
              <p></p>
            </td>
        </table> -->
                    
                
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <td width="100%" valign="middle">
               <strong> Using Context-to-Vector with Graph Retrofitting to Improve Word Embeddings</strong>
              <br><br>
              <a>Jiangbin Zheng</a>,
              <a>Yile Wang</a>,
              <a>Ge Wang</a>,
              <strong><u>Jun Xia</u></strong>,
              <a>Yufei Huang</a>,
              <a>Guojiang Zhao</a>,
              <a>Yue Zhang</a>,
              <a>Stan Z. Li</a>
              <br>
              <em>
              ACL 2022
                <br>
                <a href="https://openreview.net/forum?id=fO8IP0Lvsx"><strong>[Paper]</strong></a>
                <a href="https://openreview.net/forum?id=fO8IP0Lvsx"><strong>[Code]</strong></a>
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:uQbTnTKl_1gJ:scholar.google.com/&output=citation&scisdr=CgUSyJaOEKWStmTRdoY:AAGBfm0AAAAAYpnUbobt35sgm_oCii_1UP5KU0QiGHDi&scisig=AAGBfm0AAAAAYpnUbjZB1r8Oiafs9NEcCpHT8BlvTyAW&scisf=4&ct=citation&cd=-1&hl=en"><strong>[BibTex]</strong></a>
              <p></p>
            </td>
<!--           </tr> -->
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <td width="100%" valign="middle">
              <strong>OT Cleaner: Label Correction as Optimal Transport</strong>
              <br><br>
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=6kTV6aMAAAAJ&hl=zh-CN">Cheng Tan</a>,
              <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN">Lirong Wu</a>,
              <a href=>Yongjie Xu</a>,
             <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
 
              <em>
                ICASSP 2022
                <br>
              </em>
                <!-- <br> -->
                <!-- , <a
                  href="https://github.com/Wangt-CN/MTFN-RR-PyTorch-Code"><strong>[Code]</strong></a> -->
                <a href="https://ieeexplore.ieee.org/abstract/document/9747279"><strong>[Paper]</strong></a>
                <a href="https://github.com/junxia97/OT-Cleaner"><strong>[Code]</strong></a>
                <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:dyR6zyo_DtUJ:scholar.google.com/&output=citation&scisdr=CgUSyJaOEKWStmTRoYY:AAGBfm0AAAAAYpnUuYbfQkf87URbVQEHGdu_YuIfI-wt&scisig=AAGBfm0AAAAAYpnUuUbxHEgModfn2GmiRhzwLzHdBSZ9&scisf=4&ct=citation&cd=-1&hl=zh-CN"><strong>[BibTex]</strong></a>
              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
<!--           </tr> -->
        </table>
                    
                   
           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
           <td width="100%" valign="middle">
              <strong>Invertible Manifold Learning for Dimension Reduction</strong>
              
              <br><br>
              <a href="https://scholar.google.com/citations?user=SKTQTXwAAAAJ&hl=zh-CN">Siyuan Li</a>,
              <a href="https://scholar.google.com/citations?user=o5A23qIAAAAJ&hl=en">Haitao Lin</a>,
              <a href="https://scholar.google.com/citations?user=foERjnQAAAAJ&hl=zh-CN">Zelin Zang</a>,
              <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN">Lirong Wu</a>,
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
 
              <em>
               ECML 2021
                <br>
                <a href="https://link.springer.com/chapter/10.1007/978-3-030-86523-8_43"><strong>[Paper]</strong></a>
                <a href="https://github.com/Westlake-AI/inv-ML"><strong>[Code]</strong></a>
                 <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:I0jh8MtVggkJ:scholar.google.com/&output=citation&scisdr=CgXmD_E7EJOM5WbQ_nY:AAGBfm0AAAAAYpnV5nb3FUfTIL2l2m6xh8HayPXlUL3r&scisig=AAGBfm0AAAAAYpnV5ooNDqyVstD-v6D0qoBLJwIllxWG&scisf=4&ct=citation&cd=-1&hl=zh-CN"><strong>[BibTex]</strong></a>

              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
<!--           </tr> -->
        </table>
        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <td width="100%" valign="middle">
              <strong>Generalized Clustering and Multi-Manifold Learning With Geometric Structure Preservation</strong>
              <br><br>
              <a href="https://scholar.google.com/citations?user=Tk7TrCoAAAAJ&hl=zh-CN">Lirong Wu</a>,
              <a href="https://scholar.google.com/citations?user=EwMGZsgAAAAJ&hl=zh-CN">Zicheng Liu</a>,
              <strong><u>Jun Xia</u></strong>,
              <a href="https://scholar.google.com/citations?user=foERjnQAAAAJ&hl=zh-CN">Zelin Zang</a>,
              <a href="https://scholar.google.com/citations?user=SKTQTXwAAAAJ&hl=zh-CN">Siyuan Li</a>,
              <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ&hl=zh-CN">Stan Z.Li</a>
              <br>
              <em>
                WACV 2022
                <br>
              </em>
                <!-- <br> -->

<!--                 <em>
                <strong>
                  <font color="#a82e2e">(Best Paper Candidate)</font>
                </strong></em>  -->
                <br>
                <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Wu_Generalized_Clustering_and_Multi-Manifold_Learning_With_Geometric_Structure_Preservation_WACV_2022_paper.pdf"><strong>[Paper]</strong></a>
                <a href="https://github.com/LirongWu/GCML"><strong>[Code]</strong></a>
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:1QaOpmdUqLkJ:scholar.google.com/&output=citation&scisdr=CgUSyJaOEKWStmTTA-w:AAGBfm0AAAAAYpnWG-zVF0Yj8qKAHknxjeeeqz4QkAMU&scisig=AAGBfm0AAAAAYpnWG-vHRu0ZRvjBjA3aYXSvW6DYN7KN&scisf=4&ct=citation&cd=-1&hl=en"><strong>[BibTex]</strong></a>
                  <br><br> 

              <p></p>
              <!-- <p>In this paper, we propose a novel framework for image-text matching that achieves remarkable
                matching performance with acceptable model complexity and much less time consuming.
              </p> -->
            </td>
<!--           </tr> -->
        </table>
 
        <p></p>
        <p></p>
        <p></p>
        <p></p>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Academic Services</heading>
              <div style="line-height:25px">
                <br/>
                <strong>Conference Committee Memeber:</strong>
                <ul>
                  <li> ICML 2022-2023, NeurIPS 2022, KDD 2023, WACV 2023
                </ul>
              <strong>Journal Reviewer:</strong>
                <ul>
                  <li>
                     Neural Networks.
               </ul>  
              </div>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading> Selected Honors and Awards</heading>
              <div style="line-height:25px">
                <p>
                  <li>
                    Westlake Presidential Awards, 2023.1.<br />
                  <li>
                    National Scholarship, 2022.10.<br />
                  <li>
                    ICML 2022 Participation Grant, 2022.06.<br />
                  <li>
                    Outstanding student cadre, Zhejiang University, 2021.09<br />
                  <li>
                    Outstanding student, Zhejiang University, 2021.09<br />
                  <li>
                    Outstanding graduate of Central South University, 2020.06<br />
                  <li>
                    The Interdisciplinary Contest in Modeling (ICM) of America, Meritorious Winner (The First Prize), 2019.04<br />
                  <li>
                    World Robot Competition in World Robot Conference (WRC) 2018, The Third Prize, 2018.08<br />
                  <li>
                    The first class Scholarship, Central South University, 2017, 2018 & 2019<br />
                  <li>
                    Outstanding student, Central South University, 2017, 2018 & 2019<br />
                  <li>
                    National Scholarship, 2017.09<br />
                </p>
              </div>
            </td>
          </tr>
        </table>
        <center>
          <a href="https://clustrmaps.com/site/1bmcr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=xkIpHOAjn18cEy2DA8bdPBhIuzDEH3-rpKPaJAIdQXw&cl=ffffff" /></a>
        </center>
</body>

</html>
